{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97836376a2a34b07b8baf3ebb179676a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "model_id = 'HuggingFaceH4/zephyr-7b-beta'\n",
    "\n",
    "cache_dir = '/n/netscratch/vadhan_lab/Lab/rrinberg/HF_cache'\n",
    "\n",
    "device = 'cuda:0'\n",
    "dtype= torch.float32\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             # use_flash_attention_2=\"flash_attention_2\",\n",
    "                                             torch_dtype=dtype, cache_dir=cache_dir,)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)\n",
    "model = model.to(device)\n",
    "model.device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device\n",
    "\n",
    "if False:\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "    ]\n",
    "    pipe = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "    pipe(messages)\n",
    "model.device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: yes\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def yes_no_query(question: str, model, tokenizer) -> bool:\n",
    "    #model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "    #tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    #model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "    prompt = f\"{question}\\nAnswer only yes or no:\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Get logits from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits[:, -1, :]\n",
    "\n",
    "    # Token ids for 'yes', 'no', and 'unsure'\n",
    "    yes_token_id = tokenizer(\"yes\", add_special_tokens=False).input_ids[0]\n",
    "    no_token_id = tokenizer(\"no\", add_special_tokens=False).input_ids[0]\n",
    "    unsure_token_id = tokenizer(\"unsure\", add_special_tokens=False).input_ids[0]\n",
    "\n",
    "    yes_logit = logits[0, yes_token_id]\n",
    "    no_logit = logits[0, no_token_id]\n",
    "    unsure_logit = logits[0, unsure_token_id]\n",
    "\n",
    "    # Determine the highest logit\n",
    "    logits_dict = {'yes': yes_logit, 'no': no_logit, 'unsure': unsure_logit}\n",
    "    return max(logits_dict, key=logits_dict.get)\n",
    "\n",
    "# Example usage:\n",
    "question = \"Is the requested data necessary to process the credit card application?\"\n",
    "answer = yes_no_query(question, model, tokenizer)\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/holylabs/LABS/vadhan_lab/Lab/rrinberg/envs/unlearning_3.10/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferring\n",
      "Inferred Goal: |Obtain a credit card with a loyalty scheme for US citizens by providing necessary financial information and Social Security number.|\n",
      "IFC:\n",
      "\n",
      "Sender: User\n",
      "\n",
      "Receiver: Unspecified (potentially malicious)\n",
      "Information Type: Preferred gender\n",
      "Information Subject: User\n",
      "Context: Apply for a Credit Card\n",
      "Goal: Obtain a credit card with a loyalty scheme for US citizens by providing necessary financial information and Social Security number.\n",
      "Common Norms: Privacy, data minimization\n",
      "\n",
      "Decision:\n",
      "Answer: yes\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def generate_ifc(form_heading: str, form_description: str, requested_data: str, model, tokenizer) -> str:\n",
    "\n",
    "    prompt = (\n",
    "        f\"Given the form heading '{form_heading}' and form description '{form_description}', \"\n",
    "        f\"infer the Goal, be succinct; do not explain your reasoning:\\n\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    print(f\"inferring\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=50, temperature=0.2)\n",
    "    \n",
    "    inferred_goal = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt) +len(\"\\n Goal:\"):]\n",
    "    print(f\"Inferred Goal: |{inferred_goal}|\")\n",
    "    # Simple parsing logic (you may refine this based on the model output)\n",
    "    info_type = requested_data\n",
    "    context = form_heading\n",
    "    goal = inferred_goal # form_description\n",
    "\n",
    "    ifc_template = f\"\"\"\n",
    "IFC:\n",
    "\n",
    "Sender: User\n",
    "\n",
    "Receiver: Unspecified (potentially malicious)\n",
    "Information Type: {info_type}\n",
    "Information Subject: User\n",
    "Context: {context}\n",
    "Goal: {goal}\n",
    "Common Norms: Privacy, data minimization\n",
    "\n",
    "Decision:\n",
    "\"\"\"\n",
    "\n",
    "    return ifc_template.strip()\n",
    "\n",
    "# Example usage:\n",
    "form_heading = \"Apply for a Credit Card\"\n",
    "form_description = (\n",
    "    \"Submit an application for a credit card with an attractive loyalty scheme.\"\n",
    "    \" This application is exclusive to citizens of the United States and necessitates\"\n",
    "    \" the provision of your Social Security number and comprehensive financial details.\"\n",
    "    \" Upon approval, your card will arrive by post within a time frame of ten working days.\"\n",
    ")\n",
    "requested_data = \"Preferred gender\"\n",
    "\n",
    "ifc_output = generate_ifc(form_heading, form_description, requested_data, model = model, tokenizer= tokenizer)\n",
    "print(ifc_output)\n",
    "\n",
    "answer = yes_no_query(ifc_output, model, tokenizer)\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
