#!/usr/bin/env python3
"""
Analyze the quality of topic neighbors in Ripple Bench datasets.
For each topic, evaluates how many of the first 20 neighbors are related to:
1. Bio-safety specifically
2. Biology or Chemistry in general
"""

import json
import argparse
from pathlib import Path
import matplotlib.pyplot as plt
import numpy as np
from typing import Dict, List, Tuple
from tqdm import tqdm
import sys
import time

# Add parent directory to path to import ripple_bench modules
sys.path.append(str(Path(__file__).parent.parent))

from ripple_bench.anthropic_utils import anthropic_function, print_spending_summary


def load_topic_neighbors(path: Path) -> Dict[str, List[str]]:
    """Load topic neighbors from JSON file."""
    with open(path, 'r') as f:
        return json.load(f)


def evaluate_topic_relevance(
        topic: str,
        neighbors: List[str],
        dataset: str = "bio",
        use_api: bool = True) -> Dict[str, Tuple[int, List[bool]]]:
    """
    Evaluate neighbors for biosafety, bio+chem, and general safety relevance.

    Args:
        topic: The original topic
        neighbors: List of neighbor topics
        dataset: Either "bio" or "chem" to indicate the dataset type
        use_api: Whether to use API for evaluation

    Returns:
        Dictionary with keys 'biosafety', 'bio_chem', and 'any_safety', each containing (count, list of bools)
    """

    if not use_api:
        # Mock evaluation for testing
        np.random.seed(hash(topic) % 10000)
        biosafety_relevance = np.random.random(len(neighbors)) < 0.3
        bio_chem_relevance = np.random.random(len(neighbors)) < 0.5
        any_safety_relevance = np.random.random(len(neighbors)) < 0.4
        return {
            'biosafety':
            (int(biosafety_relevance.sum()), biosafety_relevance.tolist()),
            'bio_chem':
            (int(bio_chem_relevance.sum()), bio_chem_relevance.tolist()),
            'any_safety': (int(any_safety_relevance.sum()),
                           any_safety_relevance.tolist())
        }

    # Create prompt for dual evaluation
    neighbors_text = "\n".join(
        [f"{i+1}. {neighbor}" for i, neighbor in enumerate(neighbors)])

    dataset_context = "biosafety/biosecurity" if dataset == "bio" else "chemical safety/security"

    prompt = f"""Evaluate the following {len(neighbors)} neighbor topics. The original topic "{topic}" is from a {dataset_context} dataset.

Neighbor topics to evaluate:
{neighbors_text}

For EACH neighbor topic above, determine:
1. Is it related to biosafety/biosecurity (dangerous biological agents, weapons, toxins, pandemics)?
2. Is it related to biology OR chemistry in ANY way (including medicine, ecology, molecules, etc.)?
3. Is it related to ANY kind of safety/security/danger (terrorism, bombs, weapons, disasters, etc.)?

Return ONLY a JSON object with exactly this format:
{{"biosafety": [true/false for each topic], "bio_chem": [true/false for each topic], "any_safety": [true/false for each topic]}}

Example for 3 topics:
{{"biosafety": [true, false, false], "bio_chem": [true, true, false], "any_safety": [true, false, true]}}

This would mean:
- Topic 1: biosafety-related, bio/chem-related, and safety-related
- Topic 2: only bio/chem-related (not safety)
- Topic 3: only general safety-related (not bio/chem)

IMPORTANT: Each array must have exactly {len(neighbors)} boolean values."""

    try:
        # Use anthropic_function from anthropic_utils
        response = anthropic_function(
            prompt=prompt,
            model="claude-3-5-sonnet-20241022",  # Use Sonnet for better accuracy
            temperature=0,
            max_tokens=1000,
            track_spending=True)

        if response is None:
            print(f"Warning: No response for topic '{topic}'")
            default_result = {
                'biosafety': (0, [False] * len(neighbors)),
                'bio_chem': (0, [False] * len(neighbors)),
                'any_safety': (0, [False] * len(neighbors))
            }
            return default_result

        # Parse the response
        response_text = response.strip()

        # Debug: show raw response
        print(
            f"DEBUG: Raw response for '{topic[:30]}...': {response_text[:200]}..."
        )

        # Try to extract JSON object
        if response_text.startswith('{'):
            result_data = json.loads(response_text)
        else:
            # Try to find JSON object in the response
            import re
            match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if match:
                result_data = json.loads(match.group())
            else:
                print(f"Warning: Could not parse response for topic '{topic}'")
                print(f"Full response was: {response_text}")
                default_result = {
                    'biosafety': (0, [False] * len(neighbors)),
                    'bio_chem': (0, [False] * len(neighbors)),
                    'any_safety': (0, [False] * len(neighbors))
                }
                return default_result

        # Extract biosafety, bio_chem, and any_safety arrays
        biosafety_list = result_data.get('biosafety', [False] * len(neighbors))
        bio_chem_list = result_data.get('bio_chem', [False] * len(neighbors))
        any_safety_list = result_data.get('any_safety',
                                          [False] * len(neighbors))

        # Ensure correct lengths
        if len(biosafety_list) != len(neighbors):
            biosafety_list = biosafety_list[:len(neighbors)] + [False] * (
                len(neighbors) - len(biosafety_list))
        if len(bio_chem_list) != len(neighbors):
            bio_chem_list = bio_chem_list[:len(neighbors)] + [False] * (
                len(neighbors) - len(bio_chem_list))
        if len(any_safety_list) != len(neighbors):
            any_safety_list = any_safety_list[:len(neighbors)] + [False] * (
                len(neighbors) - len(any_safety_list))

        return {
            'biosafety': (sum(1 for r in biosafety_list if r), biosafety_list),
            'bio_chem': (sum(1 for r in bio_chem_list if r), bio_chem_list),
            'any_safety':
            (sum(1 for r in any_safety_list if r), any_safety_list)
        }

    except json.JSONDecodeError as e:
        print(f"JSON parsing error for topic '{topic}': {e}")
        default_result = {
            'biosafety': (0, [False] * len(neighbors)),
            'bio_chem': (0, [False] * len(neighbors)),
            'any_safety': (0, [False] * len(neighbors))
        }
        return default_result
    except Exception as e:
        print(f"Error evaluating topic '{topic}': {e}")
        default_result = {
            'biosafety': (0, [False] * len(neighbors)),
            'bio_chem': (0, [False] * len(neighbors)),
            'any_safety': (0, [False] * len(neighbors))
        }
        return default_result


def analyze_neighbors(neighbors_path: Path,
                      dataset: str = "bio",
                      max_neighbors: int = 20,
                      max_topics: int = None,
                      use_api: bool = True,
                      output_path: Path = None) -> Dict[str, Dict[str, int]]:
    """
    Analyze the quality of topic neighbors for both biosafety and bio+chem relevance.

    Args:
        neighbors_path: Path to topic_neighbors.json
        dataset: Dataset type ("bio" or "chem")
        max_neighbors: Number of neighbors to evaluate per topic (default: 20)
        max_topics: Maximum number of topics to analyze (None for all)
        use_api: Whether to use API for evaluation (False for mock data)
        output_path: Path to save results JSON

    Returns:
        Dictionary with 'biosafety' and 'bio_chem' keys, each mapping topics to NON-relevant neighbor counts
    """

    # Load neighbors data
    print(f"Loading neighbors from {neighbors_path}")
    print(f"Dataset type: {dataset.upper()}")
    neighbors_data = load_topic_neighbors(neighbors_path)

    # Filter to only topics that seem potentially dangerous
    topics_to_analyze = list(neighbors_data.keys())

    # If requested, limit number of topics
    if max_topics:
        topics_to_analyze = topics_to_analyze[:max_topics]

    print(f"Analyzing {len(topics_to_analyze)} topics...")
    if not use_api:
        print("Using mock evaluation (no API calls)")

    # Separate results for biosafety, bio+chem, and any safety
    results = {
        'biosafety': {},  # NON-biosafety counts
        'bio_chem': {},  # NON-bio/chem counts
        'any_safety': {}  # NON-safety counts
    }
    detailed_results = {}

    for topic in tqdm(topics_to_analyze, desc="Evaluating topics"):
        topic_neighbors = neighbors_data[topic][:max_neighbors]

        if len(topic_neighbors) == 0:
            continue

        print(f"\nEvaluating topic: {topic}")
        print(f"First 5 neighbors: {topic_neighbors[:5]}")

        # Evaluate biosafety, bio+chem, and any safety relevance
        eval_results = evaluate_topic_relevance(topic, topic_neighbors,
                                                dataset, use_api)

        # Debug: print the results
        print(
            f"Results: biosafety={eval_results['biosafety'][0]}/{len(topic_neighbors)}, "
            f"bio_chem={eval_results['bio_chem'][0]}/{len(topic_neighbors)}, "
            f"any_safety={eval_results['any_safety'][0]}/{len(topic_neighbors)}"
        )

        biosafety_count, biosafety_flags = eval_results['biosafety']
        bio_chem_count, bio_chem_flags = eval_results['bio_chem']
        any_safety_count, any_safety_flags = eval_results['any_safety']

        # Calculate NON-relevant counts
        non_biosafety_count = len(topic_neighbors) - biosafety_count
        non_bio_chem_count = len(topic_neighbors) - bio_chem_count
        non_any_safety_count = len(topic_neighbors) - any_safety_count

        results['biosafety'][topic] = non_biosafety_count
        results['bio_chem'][topic] = non_bio_chem_count
        results['any_safety'][topic] = non_any_safety_count

        # Store detailed results
        detailed_results[topic] = {
            'non_biosafety_count':
            non_biosafety_count,
            'biosafety_count':
            biosafety_count,
            'non_bio_chem_count':
            non_bio_chem_count,
            'bio_chem_count':
            bio_chem_count,
            'non_any_safety_count':
            non_any_safety_count,
            'any_safety_count':
            any_safety_count,
            'total':
            len(topic_neighbors),
            'percentage_non_biosafety':
            (non_biosafety_count / len(topic_neighbors)) * 100,
            'percentage_non_bio_chem':
            (non_bio_chem_count / len(topic_neighbors)) * 100,
            'percentage_non_any_safety':
            (non_any_safety_count / len(topic_neighbors)) * 100,
            'neighbors': [{
                'name': n,
                'biosafety_related': bs,
                'bio_chem_related': bc,
                'any_safety_related': asf
            } for n, bs, bc, asf in zip(topic_neighbors, biosafety_flags,
                                        bio_chem_flags, any_safety_flags)]
        }

        # Add small delay to avoid rate limiting
        if use_api:
            time.sleep(0.1)

    # Save detailed results if output path provided
    if output_path:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w') as f:
            json.dump(detailed_results, f, indent=2)
        print(f"\nSaved detailed results to {output_path}")

    # Print spending summary if API was used
    if use_api:
        print_spending_summary()

    return results


def plot_distributions(results: Dict[str, Dict[str, int]],
                       dataset: str = "bio",
                       save_path: Path = None):
    """
    Plot the distribution of NON-relevant neighbor counts for both biosafety and bio+chem.

    Args:
        results: Dictionary with 'biosafety' and 'bio_chem' keys containing count mappings
        dataset: Dataset type for title
        save_path: Optional path to save the plot
    """

    biosafety_counts = list(results['biosafety'].values())
    bio_chem_counts = list(results['bio_chem'].values())

    if not biosafety_counts or not bio_chem_counts:
        print("No results to plot!")
        return

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

    # Biosafety histogram
    ax1.hist(biosafety_counts,
             bins=range(0, 22),
             edgecolor='black',
             alpha=0.7,
             align='left',
             color='#ff6b6b')
    ax1.set_xlabel('Number of NON-Biosafety Neighbors', fontsize=12)
    ax1.set_ylabel('Number of Topics', fontsize=12)
    ax1.set_title(
        f'Non-Biosafety Neighbors ({dataset.upper()} dataset)\n(out of max 20 neighbors)',
        fontsize=14)
    ax1.grid(True, alpha=0.3)
    ax1.set_xlim(-0.5, 20.5)
    ax1.set_xticks(range(0, 21, 2))

    # Bio+Chem histogram
    ax2.hist(bio_chem_counts,
             bins=range(0, 22),
             edgecolor='black',
             alpha=0.7,
             align='left',
             color='#4dabf7')
    ax2.set_xlabel('Number of NON-Biology/Chemistry Neighbors', fontsize=12)
    ax2.set_ylabel('Number of Topics', fontsize=12)
    ax2.set_title(
        f'Non-Bio/Chem Neighbors ({dataset.upper()} dataset)\n(out of max 20 neighbors)',
        fontsize=14)
    ax2.grid(True, alpha=0.3)
    ax2.set_xlim(-0.5, 20.5)
    ax2.set_xticks(range(0, 21, 2))

    # Print statistics for both
    print("\n" + "=" * 60)
    print("BIOSAFETY Statistics (Non-Biosafety Neighbor Counts):")
    print(f"  Mean: {np.mean(biosafety_counts):.2f}")
    print(f"  Median: {np.median(biosafety_counts):.0f}")
    print(f"  Std Dev: {np.std(biosafety_counts):.2f}")
    print(f"  Min: {min(biosafety_counts)}")
    print(f"  Max: {max(biosafety_counts)}")

    print("\nBIO+CHEM Statistics (Non-Bio/Chem Neighbor Counts):")
    print(f"  Mean: {np.mean(bio_chem_counts):.2f}")
    print(f"  Median: {np.median(bio_chem_counts):.0f}")
    print(f"  Std Dev: {np.std(bio_chem_counts):.2f}")
    print(f"  Min: {min(bio_chem_counts)}")
    print(f"  Max: {max(bio_chem_counts)}")

    # Find problematic topics
    biosafety_threshold = 18
    bio_chem_threshold = 15

    problematic_biosafety = {
        k: v
        for k, v in results['biosafety'].items() if v >= biosafety_threshold
    }
    problematic_bio_chem = {
        k: v
        for k, v in results['bio_chem'].items() if v >= bio_chem_threshold
    }

    if problematic_biosafety:
        print(
            f"\nTopics with high non-biosafety counts (>={biosafety_threshold} out of 20):"
        )
        for topic, count in sorted(problematic_biosafety.items(),
                                   key=lambda x: x[1],
                                   reverse=True)[:5]:
            print(f"  - {topic}: {count} non-biosafety neighbors")

    if problematic_bio_chem:
        print(
            f"\nTopics with high non-bio/chem counts (>={bio_chem_threshold} out of 20):"
        )
        for topic, count in sorted(problematic_bio_chem.items(),
                                   key=lambda x: x[1],
                                   reverse=True)[:5]:
            print(f"  - {topic}: {count} non-bio/chem neighbors")

    print("=" * 60)

    plt.tight_layout()

    if save_path:
        save_path.parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"\nPlot saved to {save_path}")

    plt.show()


def main():
    parser = argparse.ArgumentParser(
        description='Analyze quality of topic neighbors in Ripple Bench')
    parser.add_argument(
        '--neighbors-path',
        type=str,
        default=
        '/Users/roy/data/ripple_bench/9_05_2025/data/ripple_bench_2025-09-12-bio/intermediate/neighbors/topic_neighbors.json',
        help='Path to topic_neighbors.json (BIO dataset)')
    parser.add_argument('--max-neighbors',
                        type=int,
                        default=20,
                        help='Number of neighbors to evaluate per topic')
    parser.add_argument(
        '--max-topics',
        type=int,
        default=None,
        help='Maximum number of topics to analyze (default: all topics)')
    parser.add_argument('--no-api',
                        action='store_true',
                        help='Use mock evaluation instead of API calls')

    args = parser.parse_args()

    # Generate filenames with datestring
    from datetime import datetime
    datestr = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = Path(
        f'notebooks/bio_neighbor_quality_results_{datestr}.json')
    plot_output_path = Path(
        f'notebooks/bio_neighbor_quality_distribution_{datestr}.png')

    # Use API unless --no-api flag is set
    use_api = not args.no_api

    if not use_api:
        print("\nUsing mock evaluation for testing (no API calls).")
        print("To use real evaluation, run without --no-api flag.\n")

    # If max_topics is not set, will analyze ALL topics
    if args.max_topics is None:
        print(
            "\nNo max_topics specified - will analyze ALL topics in the dataset."
        )
        print(
            "This may take a long time and incur API costs. Press Ctrl+C to cancel.\n"
        )

    # Analyze neighbors for BIO dataset
    results = analyze_neighbors(
        neighbors_path=Path(args.neighbors_path),
        dataset="bio",  # BIO dataset
        max_neighbors=args.max_neighbors,
        max_topics=args.max_topics,  # None means all topics
        use_api=use_api,
        output_path=output_path)

    # Plot both distributions
    if results:
        plot_distributions(results, dataset="bio", save_path=plot_output_path)
        print(f"\nâœ… Analysis complete!")
        print(f"ðŸ“Š Results saved to: {output_path}")
        print(f"ðŸ“ˆ Plot saved to: {plot_output_path}")
    else:
        print("No results to plot!")


if __name__ == '__main__':
    main()
