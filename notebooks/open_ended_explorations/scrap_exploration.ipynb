{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% [code]\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, device, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Shared training loop.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "    return model\n",
    "\n",
    "def filter_dataset(dataset, label_to_exclude):\n",
    "    \"\"\"\n",
    "    Returns a Subset of the dataset that excludes all samples with the given label.\n",
    "    Assumes the dataset has attributes `classes` (list of class names)\n",
    "    and `samples` (list of (path, label_index) tuples).\n",
    "    \"\"\"\n",
    "    if not hasattr(dataset, 'classes') or not hasattr(dataset, 'samples'):\n",
    "        raise AttributeError(\"Dataset must have 'classes' and 'samples' attributes.\")\n",
    "    \n",
    "    try:\n",
    "        # Get the index corresponding to the label to exclude.\n",
    "        exclude_idx = dataset.classes.index(label_to_exclude)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Label '{label_to_exclude}' not found in dataset classes.\")\n",
    "    \n",
    "    # Build a list of indices that do NOT have the label to exclude.\n",
    "    indices = [i for i, (_, label) in enumerate(dataset.samples) if label != exclude_idx]\n",
    "    print(f\"Filtered dataset: kept {len(indices)} out of {len(dataset.samples)} samples (excluded '{label_to_exclude}')\")\n",
    "    return Subset(dataset, indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Dataset not found in '/n/home04/rrinberg/data_dir/data_to_concept'.\n",
      "Due to licensing restrictions, the full ImageNet cannot be downloaded automatically.\n",
      "Downloading 'Imagenette' (a small ImageNet-like dataset) for demonstration purposes...\n",
      "Downloading imagenette2-160.tgz ...\n",
      "Download complete.\n",
      "Extracting files...\n",
      "Extraction complete.\n",
      "Moving downloaded dataset to '/n/home04/rrinberg/data_dir/data_to_concept'...\n",
      "Number of classes in full dataset: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/holylabs/LABS/vadhan_lab/Lab/rrinberg/envs/unlearning_3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/n/holylabs/LABS/vadhan_lab/Lab/rrinberg/envs/unlearning_3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/n/holylabs/LABS/vadhan_lab/Lab/rrinberg/envs/unlearning_3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training on the full dataset ===\n",
      "Epoch 1/10, Loss: 1.7302\n",
      "Epoch 2/10, Loss: 1.2176\n",
      "Epoch 3/10, Loss: 1.0013\n",
      "Epoch 4/10, Loss: 0.8287\n",
      "Epoch 5/10, Loss: 0.6628\n",
      "Epoch 6/10, Loss: 0.5385\n",
      "Epoch 7/10, Loss: 0.3742\n",
      "Epoch 8/10, Loss: 0.2866\n",
      "Epoch 9/10, Loss: 0.1701\n",
      "Epoch 10/10, Loss: 0.1522\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Label 'leaves' not found in dataset classes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 103\u001b[0m, in \u001b[0;36mfilter_dataset\u001b[0;34m(dataset, label_to_exclude)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# Get the index corresponding to the label to exclude.\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     exclude_idx \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel_to_exclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: 'leaves' is not in list",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 156\u001b[0m\n\u001b[1;32m    153\u001b[0m label_to_exclude \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleaves\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# <-- change this to the label you wish to exclude\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# Note: For Imagenette, class names might differ from ImageNet. Adjust accordingly.\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m filtered_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_to_exclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m filtered_loader \u001b[38;5;241m=\u001b[39m DataLoader(filtered_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Reinitialize a fresh model for training on the filtered dataset.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 105\u001b[0m, in \u001b[0;36mfilter_dataset\u001b[0;34m(dataset, label_to_exclude)\u001b[0m\n\u001b[1;32m    103\u001b[0m     exclude_idx \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mclasses\u001b[38;5;241m.\u001b[39mindex(label_to_exclude)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel_to_exclude\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found in dataset classes.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Build a list of indices that do NOT have the label to exclude.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m indices \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i, (_, label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset\u001b[38;5;241m.\u001b[39msamples) \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;241m!=\u001b[39m exclude_idx]\n",
      "\u001b[0;31mValueError\u001b[0m: Label 'leaves' not found in dataset classes."
     ]
    }
   ],
   "source": [
    "#%% [code]\n",
    "import os\n",
    "import tarfile\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "def download_and_extract(url, dest_dir, extract=True):\n",
    "    \"\"\"\n",
    "    Downloads a file from the given URL into dest_dir.\n",
    "    If extract=True and the file is a tar archive, it will extract it.\n",
    "    \"\"\"\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    filename = url.split('/')[-1]\n",
    "    file_path = os.path.join(dest_dir, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Downloading {filename} ...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # ensure we notice bad responses\n",
    "        with open(file_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        print(\"Download complete.\")\n",
    "    else:\n",
    "        print(f\"{filename} already exists at {file_path}.\")\n",
    "    \n",
    "    if extract and (file_path.endswith('.tar') or file_path.endswith('.tar.gz') or file_path.endswith('.tgz')):\n",
    "        print(\"Extracting files...\")\n",
    "        with tarfile.open(file_path, 'r:*') as tar:\n",
    "            tar.extractall(path=dest_dir)\n",
    "        print(\"Extraction complete.\")\n",
    "\n",
    "def download_dataset(dataset_dir):\n",
    "    \"\"\"\n",
    "    Checks if the dataset directory exists and contains data.\n",
    "    If not, attempts to download and extract a dataset.\n",
    "    \n",
    "    NOTE: The full ImageNet dataset cannot be auto-downloaded due to licensing.\n",
    "          Here, we use Imagenette (a smaller subset of ImageNet) as a placeholder.\n",
    "          If you have full ImageNet access, place your data in dataset_dir.\n",
    "    \"\"\"\n",
    "    # Check if dataset_dir exists and is non-empty.\n",
    "    if not os.path.exists(dataset_dir) or len(os.listdir(dataset_dir)) == 0:\n",
    "        print(f\"Dataset not found in '{dataset_dir}'.\")\n",
    "        print(\"Due to licensing restrictions, the full ImageNet cannot be downloaded automatically.\")\n",
    "        print(\"Downloading 'Imagenette' (a small ImageNet-like dataset) for demonstration purposes...\")\n",
    "        \n",
    "        # Define a URL for Imagenette (160px version) from fast.ai’s hosting.\n",
    "        imagenette_url = \"https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160.tgz\"\n",
    "        \n",
    "        # We download to the parent directory of dataset_dir.\n",
    "        parent_dir = os.path.dirname(dataset_dir.rstrip(\"/\"))\n",
    "        download_and_extract(imagenette_url, parent_dir, extract=True)\n",
    "        \n",
    "        # Imagenette extracts to a folder named \"imagenette2-160\"\n",
    "        extracted_dir = os.path.join(parent_dir, \"imagenette2-160\", \"train\")\n",
    "        if os.path.exists(extracted_dir):\n",
    "            print(f\"Moving downloaded dataset to '{dataset_dir}'...\")\n",
    "            os.rename(extracted_dir, dataset_dir)\n",
    "        else:\n",
    "            print(\"Expected extracted folder not found. Please check the extraction output.\")\n",
    "    else:\n",
    "        print(f\"Dataset found in '{dataset_dir}'.\")\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, device, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Shared training loop.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "    return model\n",
    "\n",
    "def filter_dataset(dataset, label_to_exclude):\n",
    "    \"\"\"\n",
    "    Returns a Subset of the dataset that excludes all samples with the given label.\n",
    "    Assumes the dataset has attributes `classes` (list of class names)\n",
    "    and `samples` (list of (path, label_index) tuples).\n",
    "    \"\"\"\n",
    "    if not hasattr(dataset, 'classes') or not hasattr(dataset, 'samples'):\n",
    "        raise AttributeError(\"Dataset must have 'classes' and 'samples' attributes.\")\n",
    "    \n",
    "    try:\n",
    "        # Get the index corresponding to the label to exclude.\n",
    "        exclude_idx = dataset.classes.index(label_to_exclude)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Label '{label_to_exclude}' not found in dataset classes.\")\n",
    "    \n",
    "    # Build a list of indices that do NOT have the label to exclude.\n",
    "    indices = [i for i, (_, label) in enumerate(dataset.samples) if label != exclude_idx]\n",
    "    print(f\"Filtered dataset: kept {len(indices)} out of {len(dataset.samples)} samples (excluded '{label_to_exclude}')\")\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Define data transforms – you can add more augmentation as desired.\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Set the path to your ImageNet training data.\n",
    "# For ImageNet (or your dataset), the expected folder structure is:\n",
    "#   dataset_dir/<class_name>/image.jpg\n",
    "# For demonstration, if the directory is missing or empty, Imagenette will be downloaded.\n",
    "dataset_dir = '/n/home04/rrinberg/data_dir/data_to_concept'  # <-- UPDATE this path as needed!\n",
    "download_dataset(dataset_dir)\n",
    "\n",
    "# Load the full dataset using ImageFolder.\n",
    "full_dataset = datasets.ImageFolder(root=dataset_dir, transform=transform)\n",
    "print(\"Number of classes in full dataset:\", len(full_dataset.classes))\n",
    "\n",
    "# Hyperparameters – adjust these based on your hardware.\n",
    "batch_size = 64\n",
    "\n",
    "# DataLoader for the full dataset.\n",
    "full_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "# Create a model (ResNet18 in this example) and update its final layer to match the number of classes.\n",
    "model_full = models.resnet18(pretrained=False)\n",
    "num_features = model_full.fc.in_features\n",
    "model_full.fc = nn.Linear(num_features, len(full_dataset.classes))\n",
    "model_full = model_full.to(device)\n",
    "\n",
    "# Define loss and optimizer.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_full = optim.SGD(model_full.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "print(\"=== Training on the full dataset ===\")\n",
    "model_full = train(model_full, full_loader, criterion, optimizer_full, device, num_epochs=10)\n",
    "\n",
    "# --- Now create a filtered dataset that excludes a given label ---\n",
    "label_to_exclude = \"leaves\"  # <-- change this to the label you wish to exclude\n",
    "\n",
    "# Note: For Imagenette, class names might differ from ImageNet. Adjust accordingly.\n",
    "filtered_dataset = filter_dataset(full_dataset, label_to_exclude)\n",
    "filtered_loader = DataLoader(filtered_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "# Reinitialize a fresh model for training on the filtered dataset.\n",
    "model_filtered = models.resnet18(pretrained=False)\n",
    "model_filtered.fc = nn.Linear(num_features, len(full_dataset.classes))\n",
    "model_filtered = model_filtered.to(device)\n",
    "optimizer_filtered = optim.SGD(model_filtered.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "print(f\"\\n=== Training on the filtered dataset (excluding '{label_to_exclude}') ===\")\n",
    "model_filtered = train(model_filtered, filtered_loader, criterion, optimizer_filtered, device, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n01440764',\n",
       " 'n02102040',\n",
       " 'n02979186',\n",
       " 'n03000684',\n",
       " 'n03028079',\n",
       " 'n03394916',\n",
       " 'n03417042',\n",
       " 'n03425413',\n",
       " 'n03445777',\n",
       " 'n03888257']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    # Use GPU if available.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    \n",
    "    # Data transforms – adjust or add augmentation as desired.\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    # Path to your ImageNet training data.\n",
    "    # ImageNet should be organized like: /path/to/imagenet/train/<class_name>/image.jpg\n",
    "    dataset_dir = '/path/to/imagenet/train'  # <-- UPDATE this path!\n",
    "    \n",
    "    # Load the full dataset using ImageFolder.\n",
    "    full_dataset = datasets.ImageFolder(root=dataset_dir, transform=transform)\n",
    "    print(\"Number of classes in full dataset:\", len(full_dataset.classes))\n",
    "    \n",
    "    # Adjust the batch size based on your GPU memory.\n",
    "    batch_size = 64\n",
    "    \n",
    "    # Create a DataLoader for the full dataset.\n",
    "    full_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    \n",
    "    # Create a model (ResNet18 in this example) and update its final layer to match the number of classes.\n",
    "    model_full = models.resnet18(pretrained=False)\n",
    "    num_features = model_full.fc.in_features\n",
    "    model_full.fc = nn.Linear(num_features, len(full_dataset.classes))\n",
    "    model_full = model_full.to(device)\n",
    "    \n",
    "    # Define loss and optimizer.\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_full = optim.SGD(model_full.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "    \n",
    "    print(\"=== Training on the full ImageNet dataset ===\")\n",
    "    model_full = train(model_full, full_loader, criterion, optimizer_full, device, num_epochs=10)\n",
    "    \n",
    "    # --- Now create a filtered dataset that excludes a given label ---\n",
    "    label_to_exclude = \"leaves\"  # <-- change this to the label you wish to exclude\n",
    "    filtered_dataset = filter_dataset(full_dataset, label_to_exclude)\n",
    "    filtered_loader = DataLoader(filtered_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    \n",
    "    # (Optionally) Reinitialize a fresh model for training on the filtered dataset.\n",
    "    model_filtered = models.resnet18(pretrained=False)\n",
    "    model_filtered.fc = nn.Linear(num_features, len(full_dataset.classes))\n",
    "    model_filtered = model_filtered.to(device)\n",
    "    optimizer_filtered = optim.SGD(model_filtered.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "    \n",
    "    print(f\"\\n=== Training on the filtered ImageNet dataset (excluding '{label_to_exclude}') ===\")\n",
    "    model_filtered = train(model_filtered, filtered_loader, criterion, optimizer_filtered, device, num_epochs=10)\n",
    "    \n",
    "# Run the main function – in a Jupyter Notebook, simply execute this cell.\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# download imagenet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "url = \"https://image-net.org/data/imagenet21k_resized.tar.gz\"\n",
    "session.cookies.set('PHPSESSID', '7vg2gnnfseb4fn76ilg9453os1')\n",
    "response = session.get(url, stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download of the ImageNet training set...\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: https://image-net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_img_train.tar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 81\u001b[0m\n\u001b[1;32m     78\u001b[0m dest_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/n/home04/rrinberg/data_dir/data_to_concept/image_net\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# <-- CHANGE this to your desired path\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Uncomment the line below to start downloading (note: this may take a long time!).\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m \u001b[43mdownload_imagenet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 51\u001b[0m, in \u001b[0;36mdownload_imagenet\u001b[0;34m(train_url, val_url, dest_dir)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Download training set\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting download of the ImageNet training set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m \u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_tar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Download validation set\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting download of the ImageNet validation set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m, in \u001b[0;36mdownload_file\u001b[0;34m(url, dest_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     15\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Raise an error for bad responses\u001b[39;00m\n\u001b[1;32m     17\u001b[0m total_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(response\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent-length\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     18\u001b[0m block_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m  \u001b[38;5;66;03m# 1 KB chunks\u001b[39;00m\n",
      "File \u001b[0;32m/n/holylabs/LABS/vadhan_lab/Lab/rrinberg/envs/unlearning_3.10/lib/python3.10/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://image-net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_img_train.tar"
     ]
    }
   ],
   "source": [
    "#%% [code]\n",
    "import os\n",
    "import tarfile\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_file(url, dest_path):\n",
    "    \"\"\"\n",
    "    Download a file from the given URL to the specified destination path.\n",
    "    Shows a progress bar using tqdm.\n",
    "    \"\"\"\n",
    "    if os.path.exists(dest_path):\n",
    "        print(f\"{dest_path} already exists, skipping download.\")\n",
    "        return\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()  # Raise an error for bad responses\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024  # 1 KB chunks\n",
    "    t = tqdm(total=total_size, unit='iB', unit_scale=True, desc=f\"Downloading {os.path.basename(dest_path)}\")\n",
    "    with open(dest_path, 'wb') as f:\n",
    "        for data in response.iter_content(block_size):\n",
    "            f.write(data)\n",
    "            t.update(len(data))\n",
    "    t.close()\n",
    "    if total_size != 0 and t.n != total_size:\n",
    "        print(\"ERROR, something went wrong during the download!\")\n",
    "    else:\n",
    "        print(f\"Downloaded {dest_path}.\")\n",
    "\n",
    "def extract_tarfile(file_path, extract_to):\n",
    "    \"\"\"\n",
    "    Extracts a tar (or tar.gz) archive to the specified directory.\n",
    "    \"\"\"\n",
    "    print(f\"Extracting {file_path} to {extract_to}...\")\n",
    "    with tarfile.open(file_path, 'r:*') as tar:\n",
    "        tar.extractall(path=extract_to)\n",
    "    print(\"Extraction complete.\")\n",
    "\n",
    "def download_imagenet(train_url, val_url, dest_dir):\n",
    "    \"\"\"\n",
    "    Downloads and extracts the ImageNet training and validation datasets.\n",
    "    \"\"\"\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    \n",
    "    # Define paths for the tar files\n",
    "    train_tar = os.path.join(dest_dir, os.path.basename(train_url))\n",
    "    val_tar = os.path.join(dest_dir, os.path.basename(val_url))\n",
    "    \n",
    "    # Download training set\n",
    "    print(\"Starting download of the ImageNet training set...\")\n",
    "    download_file(train_url, train_tar)\n",
    "    \n",
    "    # Download validation set\n",
    "    print(\"Starting download of the ImageNet validation set...\")\n",
    "    download_file(val_url, val_tar)\n",
    "    \n",
    "    # Extract the tar files if not already extracted.\n",
    "    # (The extraction destination may vary depending on how you want to organize your data.)\n",
    "    train_extract_path = os.path.join(dest_dir, \"ILSVRC2012_img_train\")\n",
    "    val_extract_path = os.path.join(dest_dir, \"ILSVRC2012_img_val\")\n",
    "    \n",
    "    if not os.path.exists(train_extract_path):\n",
    "        extract_tarfile(train_tar, dest_dir)\n",
    "    else:\n",
    "        print(f\"Training set already extracted to {train_extract_path}.\")\n",
    "    \n",
    "    if not os.path.exists(val_extract_path):\n",
    "        extract_tarfile(val_tar, dest_dir)\n",
    "    else:\n",
    "        print(f\"Validation set already extracted to {val_extract_path}.\")\n",
    "\n",
    "# Replace these URLs with the official download links you received from ImageNet.\n",
    "# The URLs below are the ones historically provided for ImageNet 2012.\n",
    "train_url = \"http://www.image-net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_img_train.tar\"\n",
    "val_url = \"http://www.image-net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_img_val.tar\"\n",
    "\n",
    "train_url = \"https://image-net.org/data/imagenet21k_resized.tar.gz\"\n",
    "# Update dest_dir to where you want to store the dataset.\n",
    "dest_dir = \"/n/home04/rrinberg/data_dir/data_to_concept/image_net\"  # <-- CHANGE this to your desired path\n",
    "\n",
    "# Uncomment the line below to start downloading (note: this may take a long time!).\n",
    "download_imagenet(train_url, val_url, dest_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
