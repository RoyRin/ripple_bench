#!/bin/sh
#SBATCH --job-name=evaluate_ripple
#SBATCH --array=0-11%6
#SBATCH --partition=kempner_h100
#SBATCH --mem=80G
#SBATCH --gres=gpu:1
#SBATCH -t 0-32:00
#SBATCH -c 1
#SBATCH --output=/n/home04/rrinberg/catered_out/ripple_bench/evaluate-ripple-%A__%a.out
#SBATCH --mail-user=royrinberg@gmail.com  
#SBATCH --mail-type=ALL
#SBATCH --account kempner_sham_lab

### vadhan_lab

module load python/3.10.12-fasrc01
module load intelpython/3.9.16-fasrc01
#module load cuda cudnn
mamba activate unlearning_3.10


## logging
CODE_DIR=/n/home04/rrinberg/code/data_to_concept_unlearning

## Run the experiment
cd $CODE_DIR

# Model indices:
# 0: Llama-3-8b-Instruct (base)
# 1: zephyr-7b-beta (base)
# 2: zephyr-7b-elm
# 3: zephyr-7b-rmu
# 4-11: LLM-GAT models (will evaluate all 8 checkpoints each with --all-checkpoints)

python scripts/evaluate_multiple_models.py \
    data/ripple_bench_2025-07-12_full/ripple_bench_dataset.json \
    --output-dir results/multiple_models/parallelized/ \
    --model-index $SLURM_ARRAY_TASK_ID \
    --all-checkpoints

# Total evaluations with --all-checkpoints:
# - Indices 0-3: 4 models × 1 evaluation = 4
# - Indices 4-11: 8 models × 8 checkpoints = 64
# Total: 68 model evaluations


#./evaluate_models_on_ripple.sh





