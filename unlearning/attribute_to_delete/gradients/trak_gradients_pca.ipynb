{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from unlearning import training\n",
    "from unlearning import datasets\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import wget\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn import CrossEntropyLoss, Conv2d, BatchNorm2d\n",
    "from torch.optim import SGD, lr_scheduler\n",
    "import torchvision\n",
    "import warnings\n",
    "from unlearning.datasets.cifar10 import get_cifar_dataset, get_dataloader\n",
    "from trak import TRAKer\n",
    "from unlearning.eval.nn_evals import evaluate_model\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "def get_trak_features(model, data_train_loader, ckpts):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        model (_type_): _description_\n",
    "        data_train_loader (_type_): _description_\n",
    "        ckpts (_type_): path to the checkpoint files - used for feature extraction\n",
    "    \"\"\"\n",
    "\n",
    "    traker = TRAKer(model=model,\n",
    "                    task='image_classification',\n",
    "                    proj_dim=4096,\n",
    "                    train_set_size=len(data_train_loader.dataset))\n",
    "\n",
    "    ## Compute TRAK features for train data\n",
    "    for model_id, ckpt in enumerate(tqdm(ckpts)):\n",
    "        traker.load_checkpoint(ckpt, model_id=model_id)\n",
    "        for batch in tqdm(data_train_loader):\n",
    "            batch = [x.cuda() for x in batch]\n",
    "            traker.featurize(batch=batch, num_samples=batch[0].shape[0])\n",
    "\n",
    "        traker.finalize_features()\n",
    "    return traker\n",
    "\n",
    "\n",
    "def compute_trak_scores(\n",
    "    traker,\n",
    "    trak_targets_loader,\n",
    "    save_name=\"quickstart\",\n",
    "):\n",
    "\n",
    "    for model_id, ckpt in enumerate(tqdm(ckpts)):\n",
    "        traker.start_scoring_checkpoint(exp_name=save_name,\n",
    "                                        checkpoint=ckpt,\n",
    "                                        model_id=model_id,\n",
    "                                        num_targets=len(\n",
    "                                            trak_targets_loader.dataset))\n",
    "        for batch in trak_targets_loader:\n",
    "            batch = [x.cuda() for x in batch]\n",
    "            traker.score(batch=batch, num_samples=batch[0].shape[0])\n",
    "    print(f\"finalize the scores\")\n",
    "    scores = traker.finalize_scores(exp_name=save_name)\n",
    "    print(f\"returning scores\")\n",
    "    return scores\n",
    "\n",
    "\n",
    "\n",
    "def load_model_from_checkpoints(ckpt, evaluate=False):\n",
    "    model = training.construct_rn9().to(\n",
    "        memory_format=torch.channels_last).cuda()\n",
    "    model.load_state_dict(ckpt)\n",
    "    model = model.eval()\n",
    "\n",
    "    val_loader = datasets.get_cifar_dataloader(split='val', augment=False)\n",
    "    if evaluate:\n",
    "        evaluate_model(model, val_loader)\n",
    "    return model\n",
    "\n",
    "\n",
    "do_training = False\n",
    "date_str = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "#DATA_DIR = Path(\"/n/home04/rrinberg/data_dir/unlearning\")\n",
    "\n",
    "DATA_DIR = Path(\"/n/holyscratch01/vadhan_lab/Lab/rrinberg/unlearning\")\n",
    "\n",
    "if not DATA_DIR.exists():\n",
    "    DATA_DIR = Path(os.getcwd())\n",
    "\n",
    "SAVE_DIR = DATA_DIR / \"trak_unlearning_results\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "# fix seed for torch\n",
    "\n",
    "forget_set_size = 1000\n",
    "batch_size = 128  # 512\n",
    "fulldata_checkpoints_dir = DATA_DIR / \"full_checkpoints\"\n",
    "train_dataset = get_cifar_dataset(split='train', augment=True)\n",
    "train_full_dataloader = get_dataloader(dataset=train_dataset,\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=True)\n",
    "val_dataset = get_cifar_dataset(split='val', augment=True)\n",
    "val_dataloader = get_dataloader(dataset=val_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/roy/data/unlearning/trak_results/id_1.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m id_json_1 \u001b[38;5;241m=\u001b[39m \u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrak_results_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid_1.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m experiments \u001b[38;5;241m=\u001b[39m read_json(trak_results_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiments.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m meta_data \u001b[38;5;241m=\u001b[39m read_json(trak_results_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(json_file)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_json\u001b[39m(json_file):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjson_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/unlearning-with-trak-4p5JMX0a-py3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/roy/data/unlearning/trak_results/id_1.json'"
     ]
    }
   ],
   "source": [
    "from unlearning.training.train import train_model_wrapper\n",
    "\n",
    "\n",
    "##### Train full model\n",
    "epochs = 100\n",
    "####\n",
    "## Train Models\n",
    "####\n",
    "# NOTE: we just took the train_cifar10 function and called it from `train_model_wrapper`\n",
    "print(\"training full model\")\n",
    "train_model_wrapper(model_suffix=\"full_train\",\n",
    "            epochs=epochs,\n",
    "            data_loader=train_full_dataloader,\n",
    "            checkpoints_dir=fulldata_checkpoints_dir)\n",
    "\n",
    "\n",
    "\n",
    "## acquire list of checkpoints\n",
    "##\n",
    "ckpt_files = sorted(list(Path(fulldata_checkpoints_dir).rglob('*.pt')))\n",
    "ckpts = [torch.load(ckpt, map_location='cpu') for ckpt in ckpt_files]\n",
    "last_ckpt = ckpts[-1]\n",
    "\n",
    "# load up the last model\n",
    "\n",
    "model = load_model_from_checkpoints(last_ckpt)\n",
    "\n",
    "#####\n",
    "## extract the original and the retain models\n",
    "#####\n",
    "\n",
    "batch_size = 16\n",
    "print(\"Create TRAK model\")\n",
    "\n",
    "val_traker = get_trak_features(model,\n",
    "                                    data_train_loader=train_full_dataloader,\n",
    "                                    ckpts=ckpts)\n",
    "\n",
    "traker_model = val_traker\n",
    "\n",
    "# compute TRAK scores for forget set\n",
    "\n",
    "#val_loader = datasets.get_cifar_dataloader(split='val', augment=False)\n",
    "\n",
    "print(f\"Compute TRAK on forget set\")\n",
    "val_save_name = f\"trak_scores__validation_set__{date_str}\"\n",
    "print(f\"Compute TRAK on hold out set (validation data)\")\n",
    "val_trak_scores = compute_trak_scores(traker_model,\n",
    "                                        trak_targets_loader=val_dataloader,\n",
    "                                        save_name=val_save_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load up the last model\n",
    "\n",
    "model = load_model_from_checkpoints(last_ckpt)\n",
    "print(\"retain files!\")\n",
    "\n",
    "\n",
    "##### Train retain model\n",
    "\n",
    "\n",
    "print(\"training retain model\")\n",
    "\n",
    "\n",
    "forget_dataloader, retain_dataloader = datasets.get_forget_retain_loader(\n",
    "    train_dataset,\n",
    "    forget_set_size,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    batch_size=batch_size,\n",
    "    seed=seed)\n",
    "\n",
    "retain_checkpoints_dir = DATA_DIR / \"retain_checkpoints\"\n",
    "\n",
    "train_model_wrapper(model_suffix=\"retain_train\",\n",
    "            epochs=epochs,\n",
    "            data_loader=retain_dataloader,\n",
    "            checkpoints_dir=retain_checkpoints_dir)\n",
    "##\n",
    "print(\"retain files!\")\n",
    "\n",
    "retain_ckpt_files = sorted(list(\n",
    "    Path(retain_checkpoints_dir).rglob('*.pt')))\n",
    "retain_last_ckpt = torch.load(retain_ckpt_files[-1], map_location='cpu')\n",
    "retain_model = load_model_from_checkpoints(retain_last_ckpt)\n",
    "\n",
    "\n",
    "retain_ckpt_files = sorted(list(\n",
    "    Path(retain_checkpoints_dir).rglob('*.pt')))\n",
    "retain_last_ckpt = torch.load(retain_ckpt_files[-1], map_location='cpu')\n",
    "retain_model = load_model_from_checkpoints(retain_last_ckpt)\n",
    "\n",
    "\n",
    "forget_traker = get_trak_features(model,\n",
    "                                    data_train_loader=forget_dataloader,\n",
    "                                    ckpts=ckpts)\n",
    "\n",
    "traker_model = forget_traker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlearning-with-trak-4p5JMX0a-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
