{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from itertools import cycle\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from collections import OrderedDict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from typing import List\n",
    "import itertools\n",
    "from tqdm.autonotebook import tqdm\n",
    "#from models import *\n",
    "from unlearning.unlearning_benchmarks.SCRUB import models, utils\n",
    "#import models\n",
    "\n",
    "#from logger import *\n",
    "#import utils\n",
    "\n",
    "\n",
    "from unlearning.unlearning_benchmarks.SCRUB.thirdparty.repdistiller.helper.util import adjust_learning_rate as sgda_adjust_learning_rate\n",
    "from unlearning.unlearning_benchmarks.SCRUB.thirdparty.repdistiller.distiller_zoo import DistillKL, HintLoss, Attention, Similarity, Correlation, VIDLoss, RKDLoss\n",
    "from unlearning.unlearning_benchmarks.SCRUB.thirdparty.repdistiller.distiller_zoo import PKT, ABLoss, FactorTransfer, KDSVD, FSP, NSTLoss\n",
    "\n",
    "from unlearning.unlearning_benchmarks.SCRUB.thirdparty.repdistiller.helper.loops import train_distill, train_distill_hide, train_distill_linear, train_vanilla, train_negrad, train_bcu, train_bcu_distill, validate\n",
    "\n",
    "#from unlearning.unlearning_benchmarks.SCRUB.thirdparty.repdistiller.helper.pretrain import init\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "import numpy as np\n",
    "import torch as ch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import pprint\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "\n",
    "from unlearning.auditors.utils import (\n",
    "    model_factory,\n",
    "    loader_factory,\n",
    "    load_forget_set_indices,\n",
    "    get_full_model_paths,\n",
    "    get_oracle_paths,\n",
    "    make_results_dir,\n",
    ")\n",
    "from unlearning.auditors.accuracies import eval_accuracy\n",
    "from unlearning.auditors.logit_plots import compute_logits, plot_logits\n",
    "from unlearning.auditors.basic import plot_margins\n",
    "from unlearning.auditors.ulira import cheap_ulira_audit_precomputed\n",
    "from unlearning.auditors.direct import (\n",
    "    config_submitit,\n",
    "    direct_audit_precomputed,\n",
    "    get_u_margins,\n",
    "    u_margin_job,\n",
    "    plot_margins_direct,\n",
    ")\n",
    "from unlearning.unlearning_algos.base_nn import NAME_TO_ALGO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_yaml(yaml_file):\n",
    "    with open(yaml_file, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "def load_model(path, model_factory, ds_name):\n",
    "    model = model_factory(ds_name)\n",
    "    loaded_model = ch.load(path)\n",
    "    first_key = list(loaded_model.keys())[0]\n",
    "    if \"model\" in first_key:\n",
    "        model.load_state_dict(loaded_model)\n",
    "\n",
    "    else:\n",
    "        # add \".model\" to each key in k,vs\n",
    "        loaded_model = {f\"model.{k}\": v for k, v in loaded_model.items()}\n",
    "        model.load_state_dict(loaded_model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model_and_loaders(forget_set_id= 4, ds_name = \"CIFAR10\"):\n",
    "\n",
    "    # for now, let's tie the model to the dataset, so we have fewer moving pieces\n",
    "    model = model_factory(ds_name)  # on cuda, in eval mode\n",
    "    \n",
    "    forget_set_indices = load_forget_set_indices(ds_name,forget_set_id)\n",
    "    print(f\"getting the dataloaders\")\n",
    "    with redirect_stdout(open(\"/dev/null\", \"w\")):\n",
    "        # no shuffling, no augmentation\n",
    "        train_loader = loader_factory(ds_name, indexed=False)\n",
    "        val_loader = loader_factory(ds_name, split=\"val\", indexed=False)\n",
    "        total_train_set = len(train_loader.dataset)\n",
    "        retain_inds = np.setdiff1d(np.arange(total_train_set), forget_set_indices)\n",
    "        retain_loader = loader_factory(ds_name, indices=retain_inds, indexed=False)\n",
    "\n",
    "        forget_loader = loader_factory(\n",
    "            ds_name,\n",
    "            indices=forget_set_indices,\n",
    "            batch_size=50,\n",
    "            indexed=False,\n",
    "        )\n",
    "        eval_set_inds = np.arange(\n",
    "            len(train_loader.dataset) + len(val_loader.dataset))\n",
    "        eval_loader = loader_factory(ds_name,\n",
    "                                     split=\"train_and_val\",\n",
    "                                     indices=eval_set_inds,\n",
    "                                     indexed=False)\n",
    "    # inserted by Roy for some speed reason\n",
    "    splits = [\"train\", \"val\"]\n",
    "    print(f\"loading the model!\")\n",
    "    f_ckpt_paths, f_logit_paths, f_margins_paths = get_full_model_paths(\n",
    "        ds_name, splits=splits)\n",
    "    (\n",
    "        o_ckpt_0_path,  # we only need a single oracle checkpoint\n",
    "        o_logit_paths,\n",
    "        o_margins_paths,\n",
    "    ) = get_oracle_paths(ds_name, forget_set_id, splits=splits)\n",
    "    print(f\"Loaded paths of pretrained models.\")\n",
    "\n",
    "    full_model = load_model(f_ckpt_paths[0], model_factory, ds_name)\n",
    "    return full_model, (train_loader, val_loader, forget_loader, retain_loader, eval_loader), forget_set_indices\n",
    "        \n",
    "forget_set_id = 4\n",
    "\n",
    "full_model, loaders, forget_set_indices = get_model_and_loaders(forget_set_id= forget_set_id,  ds_name = \"CIFAR10\")\n",
    "(train_loader, val_loader, forget_loader, retain_loader, eval_loader) = loaders \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "args = Namespace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.optim = 'adam'\n",
    "args.gamma = 1\n",
    "args.alpha = 0.5\n",
    "args.beta = 0\n",
    "args.smoothing = 0.5\n",
    "args.msteps = 3\n",
    "args.clip = 0.2\n",
    "args.sstart = 10\n",
    "args.kd_T = 2\n",
    "args.distill = 'kd'\n",
    "\n",
    "args.sgda_epochs = 10\n",
    "args.sgda_learning_rate = 0.0005\n",
    "args.lr_decay_epochs = [5,8,9]\n",
    "args.lr_decay_rate = 0.1\n",
    "args.sgda_weight_decay = 0.1#5e-4\n",
    "args.sgda_momentum = 0.9\n",
    "\n",
    "# TODO: teacher, student\n",
    "# teacher = None, student = None \n",
    "# TODO : datasets and dataloaders \n",
    "\n",
    "teacher = copy.deepcopy(full_model)\n",
    "student = copy.deepcopy(full_model)\n",
    "\n",
    "# load up model 1 \n",
    "model_t = copy.deepcopy(teacher)\n",
    "model_s = copy.deepcopy(student)\n",
    "\n",
    "module_list = nn.ModuleList([])\n",
    "module_list.append(model_s)\n",
    "trainable_list = nn.ModuleList([])\n",
    "trainable_list.append(model_s)\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_div = DistillKL(args.kd_T)\n",
    "criterion_kd = DistillKL(args.kd_T)\n",
    "\n",
    "\n",
    "criterion_list = nn.ModuleList([])\n",
    "criterion_list.append(criterion_cls)    # classification loss\n",
    "criterion_list.append(criterion_div)    # KL divergence loss, original knowledge distillation\n",
    "criterion_list.append(criterion_kd)     # other knowledge distillation loss\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(trainable_list.parameters(),\n",
    "                        lr=args.sgda_learning_rate,\n",
    "                        weight_decay=args.sgda_weight_decay)\n",
    "\n",
    "\n",
    "\n",
    "acc_rs = []\n",
    "acc_fs = []\n",
    "acc_ts = []\n",
    "\n",
    "\n",
    "def _adjust_learning_rate(epoch, optimizer):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by decay rate every steep step\"\"\"\n",
    "    steps = np.sum(epoch > np.asarray(args.lr_decay_epochs))\n",
    "    new_lr = args.sgda_learning_rate\n",
    "    if steps > 0:\n",
    "        new_lr = args.sgda_learning_rate * (args.lr_decay_rate ** steps)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = new_lr\n",
    "    return new_lr\n",
    "\n",
    "beta = 0.1\n",
    "def avg_fn(averaged_model_parameter, model_parameter, num_averaged): return (\n",
    "    1 - beta) * averaged_model_parameter + beta * model_parameter\n",
    "swa_model = torch.optim.swa_utils.AveragedModel(\n",
    "    model_s, avg_fn=avg_fn)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"total epochs : {args.sgda_epochs}\")\n",
    "for epoch in range(1, args.sgda_epochs + 1):\n",
    "    print(f\"Epoch {epoch} ...\")\n",
    "    lr = _adjust_learning_rate(epoch, optimizer)\n",
    "\n",
    "    print(\"==> scrub unlearning ...\")\n",
    "\n",
    "    print(f\"validating - \")\n",
    "    acc_r, acc5_r, loss_r = validate(retain_loader, model_s, criterion_cls, args, True)\n",
    "    acc_f, acc5_f, loss_f = validate(forget_loader, model_s, criterion_cls, args, True)\n",
    "    acc_rs.append(100-acc_r.item())\n",
    "    acc_fs.append(100-acc_f.item())\n",
    "\n",
    "    maximize_loss = 0\n",
    "    print(f\"train distill 1\")ÃŸ\n",
    "    if epoch <= args.msteps:\n",
    "        maximize_loss = train_distill(epoch, forget_loader, module_list, swa_model, criterion_list, optimizer, args, \"maximize\")\n",
    "    print(f\"train distill 2 :\")\n",
    "    train_acc, train_loss = train_distill(epoch, retain_loader, module_list, swa_model, criterion_list, optimizer, args, \"minimize\",)\n",
    "    if epoch >= args.sstart:\n",
    "        print(\"update params\")\n",
    "        swa_model.update_parameters(model_s)\n",
    "\n",
    "    print(\"maximize loss: {:.2f}\\t minimize loss: {:.2f}\\t train_acc: {}\".format(maximize_loss, train_loss, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
