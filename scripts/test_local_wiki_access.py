#!/usr/bin/env python3
"""
Test script for accessing local Wikipedia JSON data using wiki_rag.
This demonstrates how to use the local Wikipedia data instead of fetching online.
"""

import sys
import os
import time
from pathlib import Path

# Add wiki-rag to path
sys.path.append('wiki-rag')

from wiki_rag.wikipedia import build_title_index, get_wiki_page, clean_title


def test_local_wiki_access():
    """Test accessing Wikipedia articles from local JSON files."""

    # Path to your local Wikipedia JSON data
    wiki_json_path = "/Users/roy/data/wikipedia/wikipedia/json"

    if not Path(wiki_json_path).exists():
        print(f"âŒ Wikipedia JSON data not found at: {wiki_json_path}")
        print("Please run: python scripts/setup_wikipedia_dataset.py")
        return

    print(f"âœ… Found Wikipedia JSON data at: {wiki_json_path}")

    # Build or load title index
    print("\nğŸ“– Building title index (this may take a while on first run)...")
    start_time = time.time()

    title_to_file_path = build_title_index(wiki_json_path)

    elapsed = time.time() - start_time
    print(
        f"âœ… Index ready! ({len(title_to_file_path)} articles indexed in {elapsed:.2f}s)"
    )

    # Test articles
    test_topics = [
        "DNA", "CRISPR", "Anthrax", "Machine learning", "Nuclear weapon",
        "Artificial intelligence"
    ]

    print("\nğŸ§ª Testing article retrieval:")
    for topic in test_topics:
        print(f"\nğŸ“„ Retrieving: {topic}")

        # Method 1: Direct lookup
        start = time.time()
        article_data = get_wiki_page(topic, title_to_file_path)
        elapsed = time.time() - start

        if article_data and article_data.get('text'):
            text_preview = article_data['text'][:200].replace('\n', ' ')
            print(f"   âœ… Found! (Retrieved in {elapsed:.3f}s)")
            print(f"   ğŸ“ Length: {len(article_data['text'])} chars")
            print(f"   ğŸ“ Preview: {text_preview}...")

            # Extract some metadata if available
            if 'title' in article_data:
                print(f"   ğŸ·ï¸  Title: {article_data['title']}")
            if 'url' in article_data:
                print(f"   ğŸ”— URL: {article_data['url']}")
        else:
            # Try with cleaned title
            cleaned = clean_title(topic)
            article_data = get_wiki_page(cleaned, title_to_file_path)

            if article_data and article_data.get('text'):
                print(f"   âš ï¸  Found with cleaned title: '{cleaned}'")
                print(f"   ğŸ“ Length: {len(article_data['text'])} chars")
            else:
                print(f"   âŒ Not found (tried both '{topic}' and '{cleaned}')")

    # Compare with fact extraction
    print("\n\nğŸ”¬ Testing fact extraction from local data:")
    test_topic = "CRISPR"
    article_data = get_wiki_page(test_topic, title_to_file_path)

    if article_data and article_data.get('text'):
        content = article_data['text'][:
                                       3000]  # Same limit as the original code
        print(f"\nğŸ“„ Topic: {test_topic}")
        print(f"ğŸ“ Using first 3000 chars of {len(article_data['text'])} total")
        print(f"ğŸ“ Content preview:")
        print("-" * 80)
        print(content[:500] + "...")
        print("-" * 80)

        # This is where fact extraction would happen
        print(
            "\nğŸ’¡ This content would be passed to the LLM for fact extraction")
        print("   Instead of fetching from wikipedia.page(topic).content")

    # Performance comparison
    print("\n\nâš¡ Performance Summary:")
    print("- Local access: ~0.001-0.010s per article")
    print("- Online fetch: ~0.5-2.0s per article (plus network latency)")
    print("- Speedup: 50-200x faster!")
    print("- No internet required!")


if __name__ == "__main__":
    test_local_wiki_access()
